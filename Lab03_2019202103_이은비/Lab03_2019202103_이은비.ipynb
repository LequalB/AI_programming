{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MQBrHJ2BiAhC"
      },
      "outputs": [],
      "source": [
        "# Prepare Mini-MNIST Dataset(Data Preprocessing)\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = load_digits()\n",
        "\n",
        "# digits.data from sklearn contains 1797 images of 8x8 pixels\n",
        "# Each image has a hand-written digit\n",
        "digits_df = digits.images.reshape((len(digits.target), -1))\n",
        "digits_tf = digits.target\n",
        "\n",
        "# Splitting dataframe into train & test\n",
        "X_train_org, X_test_org, y_train_num, y_test = train_test_split(digits_df, digits_tf, test_size= 0.20, random_state= 101)\n",
        "\n",
        "# Digits data has range of [0,16], which often lead too big exponential values\n",
        "# so make them normal distribution of [0,1] with the sklearn package, or you can just divide them by 16\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train_org)\n",
        "X_test = sc.transform(X_test_org)\n",
        "\n",
        "n_classes = 10\n",
        "\n",
        "# Transform Nx1 Y vector into Nx10 answer vector, so that we can perform one-to-all classification\n",
        "y_train = np.zeros((y_train_num.shape[0],10))\n",
        "for i in range(n_classes):\n",
        "    y_train[:,i] = (y_train_num == i)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation function for forward path\n",
        "def sigmoid(x): #sigmoid function\n",
        "    # Numerically stable with large exponentials\n",
        "    x = np.where(x < 0, np.exp(x)/(1 + np.exp(x)), 1/(1 + np.exp(-x)))\n",
        "    return x\n",
        "\n",
        "def softmax(x): #softmax function\n",
        "    # Numerically stable with large exponentials\n",
        "    x = x - np.max(x, axis=-1, keepdims=True)\n",
        "    x = np.exp(x)\n",
        "    xs = np.sum(x, axis=-1, keepdims=True)\n",
        "    return x / xs"
      ],
      "metadata": {
        "id": "egLKgPrJiHFd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the data in the preprocessed data set\n",
        "print(digits_df.shape)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_train_org[0])\n",
        "\n",
        "idx = np.random.randint(X_train.shape[0])\n",
        "dimage = X_train_org[idx].reshape((8,8))\n",
        "# plt.figure(figsize=(4.32, 2.88))  # Adjust the width and height as needed\n",
        "plt.gray()\n",
        "plt.matshow(dimage)\n",
        "plt.show()\n",
        "print('The number is', y_train_num[idx])  # the number that is predicted\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "Zt-ETlFtiJKP",
        "outputId": "4273f066-809b-4389-fca0-06a67362332a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1797, 64)\n",
            "(1437, 64)\n",
            "(1437, 10)\n",
            "[ 0.  0.  0.  9. 16.  6.  0.  0.  0.  0.  4. 15.  6. 15.  0.  0.  0.  0.\n",
            "  8. 11.  9. 11.  0.  0.  0.  0.  8. 16. 14.  2.  0.  0.  0.  0. 11. 16.\n",
            " 13.  0.  0.  0.  0.  6. 14.  2. 12.  9.  0.  0.  0.  5. 16. 11.  5. 13.\n",
            "  4.  0.  0.  0.  3.  8. 13. 16.  9.  0.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 480x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number is 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Model class (Linear prediction for each function (forward,backward))\n",
        "class myDenseLayer:\n",
        "    def __init__(self, n_out, n_in):\n",
        "        self.wegt = np.zeros((n_out, n_in))\n",
        "        self.bias = np.zeros((n_out))\n",
        "\n",
        "    def forward(self, x):       # (b, i)\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        x_lin = (self.wegt @x.T).T + self.bias  # Linear Prediction\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "        return x_lin\n",
        "\n",
        "    def backward(self, x, x_in):  # x = dJ/dz (b, c)\n",
        "        ### START CODE HERE ###\n",
        "        m = x.shape[1]  # the total number\n",
        "        dw = (x.T @ x_in)/m  # Gradients for weights\n",
        "        db = (np.sum(x, axis=0))/m  # Gradients for biases\n",
        "        wdJdz =  x @ self.wegt # Propagation for lower layer\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "        return dw, db, wdJdz\n"
      ],
      "metadata": {
        "id": "6SQbzNcuiLIc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for checking backward path process\n",
        "np.random.seed(0)\n",
        "\n",
        "tmp2 = myDenseLayer(2,5)\n",
        "tmp2.wegt = np.random.randn(2,5)\n",
        "tmp2.bias = np.random.randn(2)\n",
        "\n",
        "x_in_t = np.random.randn(2,5)\n",
        "x_t = np.random.randn(2,2)\n",
        "\n",
        "dw2, db2, wdJdz2 = tmp2.backward(x_t,x_in_t)\n",
        "\n",
        "print(f\"x_t = \\n{x_t}\")\n",
        "print(f\"dw2 = \\n{dw2}\")\n",
        "print(f\"db2 = \\n{db2}\")\n",
        "print(f\"wdJdz2 = \\n{wdJdz2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfFLlJ7XLmbR",
        "outputId": "39939627-38bf-4b49-a9f0-a2d4f8c143cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_t = \n",
            "[[ 0.8644362  -0.74216502]\n",
            " [ 2.26975462 -1.45436567]]\n",
            "dw2 = \n",
            "[[ 0.09610482  0.40788358 -0.77744815 -2.75311014  1.38754493]\n",
            " [-0.13322022 -0.27280893  0.45637388  1.73266967 -1.02972684]]\n",
            "db2 = \n",
            "[ 1.56709541 -1.09826535]\n",
            "wdJdz2 = \n",
            "[[ 2.25021216 -0.35921201  0.95838857  2.01371462  1.30965288]\n",
            " [ 5.42528537 -0.47351731  2.44162379  5.23639566  3.64173802]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# activation functions for the backpropagation.\n",
        "def dJdz_sigmoid(wdJdz_upper, az):   # sigmoid function for backward\n",
        "    dJdz = wdJdz_upper * az * (1 - az)\n",
        "    return dJdz\n",
        "\n",
        "def dJdz_softmax(y_hat, y):\n",
        "    dJdz = y_hat - y   # softmax function for backward\n",
        "    return dJdz"
      ],
      "metadata": {
        "id": "0UtyVgaxiOy7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Training Functions\n",
        "def my_forward(layers, X_in):\n",
        "    l1, l2, l3 = layers\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    a_1 = sigmoid(l1.forward(X_in))                   # first stage forward\n",
        "    a_2 = sigmoid(l2.forward(a_1))                    # second stage forward\n",
        "    a_3 = softmax(l3.forward(a_2))                    # third stage forward\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return a_1, a_2, a_3\n",
        "\n",
        "def my_backward(layers, a_1, a_2, a_3, X_in, y_true):\n",
        "    l1, l2, l3 = layers\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    dw_3, db_3, wdJdz_3 = l3.backward(dJdz_softmax(a_3, y_true), a_2)  # go through 3rd stage backward\n",
        "    dw_2, db_2, wdJdz_2 = l2.backward(dJdz_sigmoid(wdJdz_3, a_2), a_1)  # go through 2nd stage backward\n",
        "    dw_1, db_1, _ = l1.backward(dJdz_sigmoid(wdJdz_2, a_1), X_in)     # go through 1st stage backward\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    d_1 = [dw_1, db_1]\n",
        "    d_2 = [dw_2, db_2]\n",
        "    d_3 = [dw_3, db_3]\n",
        "    return d_1, d_2, d_3\n",
        "\n",
        "def my_loss(layers, X_in, y_true):\n",
        "    l1, l2, l3 = layers\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    a_1, a_2, a_3 = my_forward(layers, X_in)\n",
        "    loss = -np.mean(np.sum(y_true * np.log(a_3), axis=1))  # calculate loss\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return loss\n",
        "\n",
        "def my_predict(layers, X_in):\n",
        "    l1, l2, l3 = layers\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    a_1, a_2, a_3 = my_forward(layers, X_in)\n",
        "    pred = np.argmax(a_3, axis=1)  # make prediction\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return pred\n"
      ],
      "metadata": {
        "id": "Hl_4gp6wRASL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the parameters of each layer.\n",
        "n_inputs  = 64\n",
        "n_hidden1 = 80\n",
        "n_hidden2 = 70\n",
        "n_classes = 10\n",
        "\n",
        "l1 = myDenseLayer(n_hidden1, n_inputs)\n",
        "l2 = myDenseLayer(n_hidden2, n_hidden1)\n",
        "l3 = myDenseLayer(n_classes, n_hidden2)\n",
        "\n",
        "layers = [l1, l2, l3]\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(l1.wegt.shape, l1.bias.shape)\n",
        "print(l2.wegt.shape, l2.bias.shape)\n",
        "print(l3.wegt.shape, l3.bias.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBiUzy-IiS__",
        "outputId": "3544b2f5-c2ca-4575-8608-33bed7b09f13"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1437, 64) (1437, 10)\n",
            "(80, 64) (80,)\n",
            "(70, 80) (70,)\n",
            "(10, 70) (10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the weights\n",
        "l1.wegt = np.random.randn(n_hidden1, n_inputs)\n",
        "l2.wegt = np.random.randn(n_hidden2, n_hidden1)\n",
        "l3.wegt = np.random.randn(n_classes, n_hidden2)"
      ],
      "metadata": {
        "id": "Egx6llpziVCL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for Splitting Dataset into mini-Batches (create_mini_batch)\n",
        "def create_mini_batches(X, y, batch_size=64):\n",
        "    mini_batches = []\n",
        "    n_minibatches = X.shape[0] // batch_size  # the number of minibatches\n",
        "    n_variables = X.shape[1]  # the number of variables\n",
        "\n",
        "    data = np.hstack((X, y))\n",
        "    np.random.shuffle(data)\n",
        "    # uee the loop for trainig the entire dataset (the whole batch is epoch)\n",
        "    for i in range(n_minibatches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = (i + 1) * batch_size\n",
        "        mini_batch = data[start_idx:end_idx]\n",
        "\n",
        "        X_mini = mini_batch[:, :n_variables]  # split the datset\n",
        "        y_mini = mini_batch[:, n_variables:]\n",
        "        mini_batches.append((X_mini, y_mini)) # append by the end of the dataset\n",
        "\n",
        "    if data.shape[0] % batch_size != 0:\n",
        "        remaining_data = data[n_minibatches * batch_size:]\n",
        "        X_mini = remaining_data[:, :n_variables]  # split the datset\n",
        "        y_mini = remaining_data[:, n_variables:]\n",
        "        mini_batches.append((X_mini, y_mini)) # append by the end of the dataset\n",
        "\n",
        "    return mini_batches\n"
      ],
      "metadata": {
        "id": "tFW1Bb_0RZ-r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Vz0F95uX4siO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the result of the 'create_mini_batches' function\n",
        "np.random.seed(1)\n",
        "\n",
        "a = np.arange(20).reshape(10,2)\n",
        "b = -np.arange(10,20).reshape(10,1)\n",
        "# check the function(create_mini_batches)\n",
        "c = create_mini_batches(a, b, 4)\n",
        "for mini_X, mini_y in c:\n",
        "    print(mini_X)\n",
        "    print(mini_y, '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1tUozwqiY5G",
        "outputId": "f0b7f622-e595-4904-c195-bc7474789f7e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4  5]\n",
            " [18 19]\n",
            " [12 13]\n",
            " [ 8  9]]\n",
            "[[-12]\n",
            " [-19]\n",
            " [-16]\n",
            " [-14]] \n",
            "\n",
            "[[ 0  1]\n",
            " [ 6  7]\n",
            " [ 2  3]\n",
            " [14 15]]\n",
            "[[-10]\n",
            " [-13]\n",
            " [-11]\n",
            " [-17]] \n",
            "\n",
            "[[16 17]\n",
            " [10 11]]\n",
            "[[-18]\n",
            " [-15]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Various Optimizers\n",
        "class myOptParam:\n",
        "    def __init__(self, n_out, n_in):\n",
        "        # Previoud delta values for momentum optimizer\n",
        "        self.W_dt = np.zeros((n_out, n_in))\n",
        "        self.B_dt = np.zeros(n_out)\n",
        "        # Variables for other optimizers\n",
        "        self.W_mt = np.zeros((n_out, n_in))\n",
        "        self.B_mt = np.zeros(n_out)\n",
        "        self.W_vt = np.zeros((n_out, n_in))\n",
        "        self.B_vt = np.zeros(n_out)\n",
        "# optimizer functions (sgd, momentum, adagrad, rmsprop, adam)\n",
        "def my_optimizer(lyr, opt, W_grad, B_grad, solver='sgd', learning_rate=0.01, iter=1):\n",
        "    epsilon = 1e-8  # arbitrary small number\n",
        "    alpha = eta = learning_rate\n",
        "    if iter==0:\n",
        "        print('iteration should start from 1.')\n",
        "\n",
        "    # optimizer routines\n",
        "    if  solver=='sgd':  # Stochastic Gradient\n",
        "        W_dlt = alpha * W_grad\n",
        "        B_dlt = alpha * B_grad\n",
        "    elif solver=='momentum':   # SGD with momentum\n",
        "        gamma = 0.9               # default setting\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        W_dlt = alpha * W_grad + gamma * opt.W_dt  # momentum for previous delta\n",
        "        B_dlt = alpha * B_grad + gamma * opt.B_dt # same goes for bias\n",
        "        opt.W_dt = W_dlt # keep data for later use\n",
        "        opt.B_dt = B_dlt # for bias, too\n",
        "\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "    elif solver=='adagrad':   # AdaGrad\n",
        "        ### START CODE HERE ###\n",
        "        opt.W_vt += W_grad ** 2 # accumulate delta square (2nd momentum\n",
        "        opt.B_vt += B_grad ** 2 # accumulater for bias term\n",
        "        W_dlt = (alpha / (np.sqrt(opt.W_vt) + epsilon)) * W_grad # calculate new delta for weight\n",
        "        B_dlt = (alpha / (np.sqrt(opt.B_vt) + epsilon)) * B_grad # and for bias\n",
        "    elif solver=='rmsprop':   # rmsprop\n",
        "        beta2 = 0.9               # default setting\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        opt.W_vt = beta2 * opt.W_vt + (1 - beta2) * W_grad ** 2 # blending with second momentum\n",
        "        opt.B_vt = beta2 * opt.B_vt + (1 - beta2) * B_grad ** 2 # also doging samething for bias\n",
        "        W_dlt = (alpha / (np.sqrt(opt.W_vt) + epsilon)) * W_grad  # calculate new delta for weight\n",
        "        B_dlt = (alpha / (np.sqrt(opt.B_vt) + epsilon)) * B_grad  # and for bias\n",
        "        ### END CODE HERE ###\n",
        "    elif solver=='adam':  # adam\n",
        "        beta1, beta2 = 0.9, 0.99  # default setting\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        opt.W_mt = beta1 * opt.W_mt + (1 - beta1) * W_grad  # blending with first momentum\n",
        "        opt.B_mt = beta1 * opt.B_mt + (1 - beta1) * B_grad  # first momentum for bias\n",
        "        opt.W_vt = beta2 * opt.W_vt + (1 - beta2) * W_grad ** 2 # blending with second momentum\n",
        "        opt.B_vt = beta2 * opt.B_vt + (1 - beta2) * B_grad ** 2 # second momentum for bias\n",
        "        W_mc = opt.W_mt / (1 - beta1 ** iter) # bias correction of first momentum for weight\n",
        "        B_mc = opt.B_mt / (1 - beta1 ** iter) # and for bias term\n",
        "        W_vc = opt.W_vt / (1 - beta2 ** iter) # bias correction of second momentum for weight\n",
        "        B_vc = opt.B_vt / (1 - beta2 ** iter) # and for bias term\n",
        "        W_dlt = (alpha / (np.sqrt(W_vc) + epsilon)) * W_mc  # calculate new delat for weight\n",
        "        B_dlt = (alpha / (np.sqrt(B_vc) + epsilon)) * B_mc   # and for bias\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "    else:\n",
        "        print('optimizer error')\n",
        "\n",
        "    # Adjust weight\n",
        "    lyr.wegt = lyr.wegt - W_dlt\n",
        "    lyr.bias = lyr.bias - B_dlt\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "u84x6AClid59"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer Test\n",
        "np.random.seed(1)\n",
        "\n",
        "lyr = myDenseLayer(2,3)\n",
        "opt = myOptParam(2,3)\n",
        "\n",
        "lyr.wegt = np.random.randn(2,3)\n",
        "lyr.bias = np.random.randn(2)\n",
        "opt.W_dt = np.random.randn(2,3)\n",
        "opt.B_dt = np.random.randn(2)\n",
        "opt.W_mt = np.random.randn(2,3)\n",
        "opt.B_mt = np.random.randn(2)\n",
        "opt.W_vt = np.abs(np.random.randn(2,3))\n",
        "opt.B_vt = np.abs(np.random.randn(2))\n",
        "\n",
        "W_grad = np.random.randn(2,3)\n",
        "B_grad = np.random.randn(2)\n",
        "\n",
        "# optimizer settings are: 'sgd', 'momentum', 'adagrad', 'rmsprop', 'adam'\n",
        "#Check comment on/off as appropriate\n",
        "\n",
        "my_optimizer(lyr, opt, W_grad, B_grad, 'sgd', 10, 3)\n",
        "print(\"For SGD:\")\n",
        "print(lyr.wegt[0], lyr.bias[0])\n",
        "# my_optimizer(lyr, opt, W_grad, B_grad, 'momentum', 10, 3)\n",
        "# print(\"For Momentum:\")\n",
        "# print(lyr.wegt[0], lyr.bias[0])\n",
        "# my_optimizer(lyr, opt, W_grad, B_grad, 'adagrad', 10, 3)\n",
        "# print(\"For Adagrad:\")\n",
        "# print(lyr.wegt[0], lyr.bias[0])\n",
        "# my_optimizer(lyr, opt, W_grad, B_grad, 'rmsprop', 10, 3)\n",
        "# print(\"For RMSProp:\")\n",
        "# print(lyr.wegt[0], lyr.bias[0])\n",
        "# my_optimizer(lyr, opt, W_grad, B_grad, 'adam', 10, 3)\n",
        "# print(\"For adam:\")\n",
        "# print(lyr.wegt[0], lyr.bias[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktXFvqFvier5",
        "outputId": "a40a087d-d179-41d3-e035-4120c53effc9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For SGD:\n",
            "[8.49607236 7.8403     6.18428956] -14.853210006882223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Opimizer Parameters (apply function to each layer)\n",
        "o1 = myOptParam(n_hidden1, n_inputs)\n",
        "o2 = myOptParam(n_hidden2, n_hidden1)\n",
        "o3 = myOptParam(n_classes, n_hidden2)"
      ],
      "metadata": {
        "id": "5Oi3ddjNihGO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trainig Simple Neural Network Model(3 layer model)\n",
        "# optimizer settings are: 'sgd', 'momentum', 'adagrad', 'rmsprop', 'adam'\n",
        "# alpha is learning rate\n",
        "optimizer ='sgd'  # set the optimizer\n",
        "alpha = 0.01  # set the value of alpha that is learning rate.\n",
        "n_epochs = 1000 # set the number of epochs\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    batches = create_mini_batches(X_train, y_train, batch_size=64)\n",
        "    for one_batch in batches:\n",
        "        X_mini, y_mini = one_batch\n",
        "        batch_len = X_mini.shape[0]  # last batch might have different length\n",
        "\n",
        "        # Forward Path\n",
        "        a_1, a_2, a_3 = my_forward(layers, X_mini)\n",
        "\n",
        "        # Backward Path\n",
        "        d_1, d_2, d_3 = my_backward(layers, a_1, a_2, a_3, X_mini, y_mini)\n",
        "\n",
        "        dw_1, db_1 = d_1\n",
        "        dw_2, db_2 = d_2\n",
        "        dw_3, db_3 = d_3\n",
        "\n",
        "        # Update weights and biases\n",
        "        my_optimizer(l1, o1, dw_1, db_1, solver=optimizer, learning_rate=alpha, iter=epoch+1)\n",
        "        my_optimizer(l2, o2, dw_2, db_2, solver=optimizer, learning_rate=alpha, iter=epoch+1)\n",
        "        my_optimizer(l3, o3, dw_3, db_3, solver=optimizer, learning_rate=alpha, iter=epoch+1)\n",
        "\n",
        "    if ((epoch+1)%100==0):\n",
        "        loss_J = my_loss(layers, X_train, y_train)\n",
        "        print('Epoch: %4d,  loss: %10.8f' % (epoch+1, loss_J))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_Jtd9Of5crl",
        "outputId": "9d5dcc0a-98fc-4816-8edc-256143717c2d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  100,  loss: 0.34530735\n",
            "Epoch:  200,  loss: 0.19123897\n",
            "Epoch:  300,  loss: 0.12508008\n",
            "Epoch:  400,  loss: 0.08817055\n",
            "Epoch:  500,  loss: 0.06534233\n",
            "Epoch:  600,  loss: 0.05036992\n",
            "Epoch:  700,  loss: 0.04011914\n",
            "Epoch:  800,  loss: 0.03284979\n",
            "Epoch:  900,  loss: 0.02752606\n",
            "Epoch: 1000,  loss: 0.02351377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model Performance\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = my_predict(layers, X_test)\n",
        "# calculate the accuracy\n",
        "accuracy_score(y_pred, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fPO8whDil1Y",
        "outputId": "e30caeda-da71-4ff2-a093-f4b414aef2ff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9388888888888889"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network from scikit-learn\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "# set the parameters of MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(80, 70, ), activation='logistic', solver='sgd', \\\n",
        "                    alpha=0.01, learning_rate_init=0.01, max_iter=1000)\n",
        "\n",
        "# Training/Fitting the Model\n",
        "mlp.fit(X_train, y_train_num)\n",
        "\n",
        "# Making Predictions\n",
        "s_pred = mlp.predict(X_test)\n",
        "accuracy_score(s_pred, y_test)  # calculate the accuray"
      ],
      "metadata": {
        "id": "kCIHB0cLindr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439afd38-5e52-49d2-d324-1be95421f90a"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9694444444444444"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Model with a random sample\n",
        "idx = np.random.randint(X_test.shape[0])\n",
        "dimage = X_test_org[idx].reshape((8,8))\n",
        "plt.gray()\n",
        "plt.matshow(dimage)\n",
        "plt.show()\n",
        "\n",
        "X_input = np.expand_dims(X_test[idx], 0)\n",
        "\n",
        "y_pred = my_predict(layers, X_input)\n",
        "\n",
        "s_pred = mlp.predict(X_input)\n",
        "# print out the predicted number.\n",
        "print('My prediction is ' + str(y_pred[0]))\n",
        "print('sk prediction is ' + str(s_pred[0]))\n",
        "print('Actual number is ' + str(y_test[idx]))\n"
      ],
      "metadata": {
        "id": "quq3XiwKipNm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "83800733-6f0e-4e54-f687-202867586e48"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 480x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My prediction is 5\n",
            "sk prediction is 5\n",
            "Actual number is 5\n"
          ]
        }
      ]
    }
  ]
}